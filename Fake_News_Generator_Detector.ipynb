{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Project\n",
    "## Group-05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fake News Generation and Detection Using ANN and Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import system, name\n",
    "from IPython.display import clear_output\n",
    "import string\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4009, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URLs</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bbc.com/news/world-us-canada-414191...</td>\n",
       "      <td>Four ways Bob Corker skewered Donald Trump</td>\n",
       "      <td>Image copyright Getty Images\\nOn Sunday mornin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.reuters.com/article/us-filmfestiva...</td>\n",
       "      <td>Linklater's war veteran comedy speaks to moder...</td>\n",
       "      <td>LONDON (Reuters) - “Last Flag Flying”, a comed...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nytimes.com/2017/10/09/us/politics...</td>\n",
       "      <td>Trump’s Fight With Corker Jeopardizes His Legi...</td>\n",
       "      <td>The feud broke into public view last week when...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.reuters.com/article/us-mexico-oil-...</td>\n",
       "      <td>Egypt's Cheiron wins tie-up with Pemex for Mex...</td>\n",
       "      <td>MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.cnn.com/videos/cnnmoney/2017/10/08/...</td>\n",
       "      <td>Jason Aldean opens 'SNL' with Vegas tribute</td>\n",
       "      <td>Country singer Jason Aldean, who was performin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                URLs  \\\n",
       "0  http://www.bbc.com/news/world-us-canada-414191...   \n",
       "1  https://www.reuters.com/article/us-filmfestiva...   \n",
       "2  https://www.nytimes.com/2017/10/09/us/politics...   \n",
       "3  https://www.reuters.com/article/us-mexico-oil-...   \n",
       "4  http://www.cnn.com/videos/cnnmoney/2017/10/08/...   \n",
       "\n",
       "                                            Headline  \\\n",
       "0         Four ways Bob Corker skewered Donald Trump   \n",
       "1  Linklater's war veteran comedy speaks to moder...   \n",
       "2  Trump’s Fight With Corker Jeopardizes His Legi...   \n",
       "3  Egypt's Cheiron wins tie-up with Pemex for Mex...   \n",
       "4        Jason Aldean opens 'SNL' with Vegas tribute   \n",
       "\n",
       "                                                Body  Label  \n",
       "0  Image copyright Getty Images\\nOn Sunday mornin...      1  \n",
       "1  LONDON (Reuters) - “Last Flag Flying”, a comed...      1  \n",
       "2  The feud broke into public view last week when...      1  \n",
       "3  MEXICO CITY (Reuters) - Egypt’s Cheiron Holdin...      1  \n",
       "4  Country singer Jason Aldean, who was performin...      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Open data set from Kaggle: https://www.kaggle.com/jruvika/fake-news-detection[1]\n",
    "#Overview of the data set\n",
    "column= ['URLs', 'Headline', 'Body', 'Label']\n",
    "dataset = pd.read_csv(\"data.csv\",header=None, skiprows=1, names=column)\n",
    "print(dataset.shape)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fake News Generation using ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in only the two columns we need \n",
    "news = pd.read_csv('data.csv', usecols = ['Body', 'Label'])  #Labels: 0 for fake, 1 for true\n",
    "news['Body'] = news['Body'].astype(str)\n",
    "\n",
    "#removes noises\n",
    "final_words = [re.sub('[^0-9a-zA-Z .]+', '', word) for word in news['Body']]\n",
    "news['Body'] = final_words\n",
    "\n",
    "#samples\n",
    "np.savetxt(r'news.txt', news.loc[[7, 12], :].values, fmt='%s')\n",
    "file_name = \"news.txt\"\n",
    "news_text = open(file_name).read()\n",
    "news_text = news_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 ANN initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  6008\n",
      "Total Vocabulary:  38\n",
      "Total Patterns:  5908\n",
      "Please wait...\n",
      "Epoch 1/1\n",
      "5908/5908 [==============================] - 288s 49ms/step - loss: 3.0083\n",
      "ANN saved successfully in textgen_nn_weights.hdf5!\n"
     ]
    }
   ],
   "source": [
    "#creates a mapping of unique chars to integers\n",
    "unique_chars = sorted(list(set(news_text)))\n",
    "c_to_i_map = dict((char, intgr) for intgr, char in enumerate(unique_chars))\n",
    "total_chars = len(news_text)\n",
    "total_vocab = len(unique_chars)\n",
    "\n",
    "print(\"Total Characters: \", total_chars)\n",
    "print(\"Total Vocabulary: \", total_vocab)\n",
    "\n",
    "#prepare the dataset of input to output pairs encoded as integers\n",
    "X_temp = []\n",
    "Y_temp = []\n",
    "sq_len = 100 #splits news_text into subsequences with 100 characters \n",
    "\n",
    "for i in range(0, total_chars - sq_len, 1):\n",
    "    sq_inp = news_text[i:i + sq_len]\n",
    "    sq_out = news_text[i + sq_len]\n",
    "    X_temp.append([c_to_i_map[ch] for ch in sq_inp])\n",
    "    Y_temp.append(c_to_i_map[sq_out])\n",
    "\n",
    "total_patterns = len(X_temp)\n",
    "print(\"Total Patterns: \", total_patterns)\n",
    "\n",
    "X = numpy.reshape(X_temp, (total_patterns, sq_len, 1)) #converts X to [samples, time steps, features] for LSTM\n",
    "X = X / float(total_vocab)                             #normalizes X\n",
    "Y = np_utils.to_categorical(Y_temp)                    #one hot encode y\n",
    "\n",
    "#LSTM\n",
    "textgen_model = Sequential()\n",
    "textgen_model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "textgen_model.add(Dropout(0.2))\n",
    "textgen_model.add(LSTM(256))\n",
    "textgen_model.add(Dropout(0.2))\n",
    "textgen_model.add(Dense(Y.shape[1], activation='softmax'))\n",
    "textgen_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "filepath=\"textgen_nn_weights.hdf5\"    #saves file to create checkpoints for each epoch\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "print('Please wait...')\n",
    "textgen_model.fit(X, Y, epochs=100, batch_size=64, callbacks=callbacks_list)\n",
    "\n",
    "print('ANN saved successfully in textgen_nn_weights.hdf5!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 News text prediction and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "\n",
      "Generated News:\n",
      "\n",
      "['to learn in life and in baseball. youve had your 15 minutes of fame and brought a bunch of drama int o the game i love. now who is next because it is coming. the newbie opened up the flood gates.so now that its out there and instead of the news main focus being about the yankees blowing two games aga'\n",
      " ' regular season. maxwell said his decision had been coming for a long time and referenced what happe ned in charlottesville. hes had a while to sit and stew on that and just now decides to join the movement and take a knee right at the end of the season. i dont look at that and find that to be brave '\n",
      " 'ed so hard to avoid in high school. i respect your opinion whatever it may be but i want to see you  play the game and hear about all of the news that comes form that..not from you taking a knee and causing americans everywhere to debate about whether it is right or wrong.that is why i stopped watchi'\n",
      " 'or late last year.for the full mint interview see bit.ly2yytqpnreporting by suvashree dey choudhury  1\\njust shut up  play some damn baseballbefore its newslook plain and simple i am over this take a knee bullshit in my opinion.kyle teradausa today sportsim not writing to debate with anyone on this. e'\n",
      " 'job is to focus on playing baseball right now. it is so thank you judge. even cc sabathia had an opi nion on it and he made that pretty clear on saturday but he said his peace and left it at that. so when sunday came around he didnt want to revisit it and said i dont want to talk about that. go to so']\n"
     ]
    }
   ],
   "source": [
    "######generates 5 news texts\n",
    "#reloads network weights\n",
    "print(\"Please wait...\")\n",
    "filename = \"textgen_nn_weights.hdf5\"\n",
    "textgen_model.load_weights(filename)\n",
    "textgen_model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "text_generated = []\n",
    "i_to_c_map = dict((intgr, char) for intgr, char in enumerate(unique_chars))\n",
    "\n",
    "for j in range(0, 5):\n",
    "    #random index to start\n",
    "    start_index = numpy.random.randint(0, len(X_temp)-1)\n",
    "    pattern = X_temp[start_index]\n",
    "    text_temp = ''.join([i_to_c_map[v] for v in pattern])\n",
    "\n",
    "    #generates next characters\n",
    "    new_text = \"\"\n",
    "    for i in range(200):\n",
    "        x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "        x = x / float(total_vocab)\n",
    "        predict = textgen_model.predict(x, verbose=0)\n",
    "        indx = numpy.argmax(predict)\n",
    "        result = i_to_c_map[indx]\n",
    "        sq_inp = [i_to_c_map[v] for v in pattern]\n",
    "        new_text += result\n",
    "        pattern.append(indx)\n",
    "        pattern = pattern[1:len(pattern)]    \n",
    "\n",
    "    text_generated.append(text_temp + ' ' + new_text)    \n",
    "\n",
    "yTest = [0, 1, 0, 1, 0]                                #assigns random classes. 0 for fake. 1 for true\n",
    "print(\"\\nGenerated News:\\n\")\n",
    "print(numpy.array(text_generated))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Fake News Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fake News Detection using ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding \n",
    "from keras.layers import Input,LSTM,Bidirectional,Activation,Conv1D\n",
    "from keras.layers import Embedding,GlobalMaxPooling1D, MaxPooling1D, Add\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Shape of data tensor: (2806, 100)\n",
      "Shape of label tensor: (2806,)\n",
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#reads csv file\n",
    "print(\"Please wait...\")\n",
    "dataset = pd.read_csv('data.csv', usecols = ['Body', 'Label'])\n",
    "dataset['Body'] = dataset['Body'].astype('str')\n",
    "#dataset.head()\n",
    "\n",
    "#train data and test data seperation\n",
    "Train_data= dataset.Body\n",
    "labels = dataset.Label\n",
    "X_train,X_test,y_train,y_test = train_test_split(Train_data,labels,test_size=0.30,random_state=0)\n",
    "\n",
    "#tokenization and padding\n",
    "max_length=100\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(X_train) #creates the vocabulary index based on word frequency\n",
    "vocab_size = len(tok.word_index) + 1\n",
    "#integer encoding\n",
    "X_train_encoded = tok.texts_to_sequences(X_train)#takes each word in the text and replaces \n",
    "                                                 #it with its corresponding integer value\n",
    "\n",
    "x_train_padding=pad_sequences(X_train_encoded, maxlen=max_length, padding='post') #pad_sequences is used to ensure that all sequences in a \n",
    "                                                                                  #list have the same length\n",
    "print('Shape of data tensor:', x_train_padding.shape)\n",
    "print('Shape of label tensor:', y_train.shape)\n",
    "\n",
    "#encodes and pads X_test\n",
    "X_test_encoded = tok.texts_to_sequences(X_test)\n",
    "x_test_padding=sequence.pad_sequences(X_test_encoded, maxlen=max_length,padding='post')\n",
    "\n",
    "#loads embedding\n",
    "embeddings_index = dict()\n",
    "#glove word emedding\n",
    "f = open('glove.6B.100d.txt',encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "#initializes a matrix with zeros having dimensions equivalent to vocab size and 100\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "#iterates over words and indexes in the data\n",
    "for word, idx_word in tok.word_index.items():\n",
    "    #gets embedding vectors\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        #inserts it in the matrix at the index of the word\n",
    "        embedding_matrix[idx_word] = embedding_vector\n",
    "#print (embedding_matrix[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 ANN initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please wait...\n",
      "\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 100, 100)          4748600   \n",
      "_________________________________________________________________\n",
      "flatten_5 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 4,758,601\n",
      "Trainable params: 10,001\n",
      "Non-trainable params: 4,748,600\n",
      "_________________________________________________________________\n",
      "None\n",
      "Accuracy history:  [0.7947255882380778, 0.9208838203848895, 0.9586600142551674, 0.9707769065012017, 0.9782608694377662]\n",
      "\n",
      "Validation history:  [0.8894430587713855, 0.9201995015937094, 0.9368246051537822, 0.9376558603491272, 0.940980881130507]\n",
      "\n",
      "ANN saved successfully in nn_detector_weights.hdf5!\n"
     ]
    }
   ],
   "source": [
    "#defines NN model\n",
    "print(\"\\nPlease wait...\\n\")\n",
    "\n",
    "nn_model = Sequential()\n",
    "#this layer can only be used as the first layer in a model\n",
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False) \n",
    "nn_model.add(e)                                                                                      \n",
    "nn_model.add(Flatten())\n",
    "nn_model.add(Dense(1, activation='sigmoid'))\n",
    "nn_model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['acc'])\n",
    "print(nn_model.summary())\n",
    "\n",
    "#fits the model\n",
    "history=nn_model.fit(x_train_padding, y_train, epochs=5, verbose=0, validation_data=(x_test_padding,y_test))\n",
    "#saves network weights for future use\n",
    "nn_model.save_weights('nn_detector_weights.hdf5')\n",
    "\n",
    "#accuracies and losses\n",
    "acc = history.history['acc']\n",
    "print (\"Accuracy history: \",acc)\n",
    "val_acc = history.history['val_acc']\n",
    "print(\"\\nValidation history: \",val_acc)\n",
    "\n",
    "print('\\nANN saved successfully in nn_detector_weights.hdf5!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3 Classification of newly generated news by the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifications predicted by NN: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "#reloads network weights\n",
    "filename = \"nn_detector_weights.hdf5\"\n",
    "nn_model.load_weights(filename)\n",
    "nn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "yTestNN = []\n",
    "for i in range(0, len(text_generated)):\n",
    "    news = [[text_generated[i], yTest[i]]]\n",
    "    Test = pd.DataFrame(news, columns = ['News', 'Label'])\n",
    "    sentences_test = Test.News\n",
    "    y_test = Test.Label\n",
    "    X_test = tok.texts_to_sequences(sentences_test)\n",
    "    X_test = pad_sequences(X_test, padding='post', maxlen=max_length)\n",
    "    \n",
    "    loss, accuracy = nn_model.evaluate(X_test, y_test, verbose=False)\n",
    "    \n",
    "    if(accuracy < 0.5):\n",
    "        if(yTest[i] == 0):\n",
    "            yTestNN.append(1)\n",
    "        else:\n",
    "            yTestNN.append(0)\n",
    "    else:\n",
    "        yTestNN.append(yTest[i])\n",
    "        \n",
    "print('Classifications predicted by NN:', yTestNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Fake News Detection Using Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "\n",
      "Total vocabulary:  11287\n",
      "XTrain shape: (2806, 11287)\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait...\\n\")\n",
    "#reads csv file\n",
    "column= ['URLs', 'Headline', 'Body', 'Label']\n",
    "dataset = pd.read_csv(\"data.csv\",header=None, skiprows=1, names=column)\n",
    "dataset['Body'] = dataset['Body'].astype('str')\n",
    "\n",
    "#removes noises\n",
    "final_words = [re.sub('[^0-9a-zA-Z .]+', '', word) for word in dataset['Body']]\n",
    "dataset['Body'] = final_words\n",
    "\n",
    "#training and test set seperation\n",
    "Train_data= dataset.Body\n",
    "labels = dataset.Label\n",
    "X_train,X_test,y_train,y_test = train_test_split(Train_data,labels,test_size=0.30,random_state=0)\n",
    "\n",
    "#toeknization\n",
    "def myTokenizer(textData):\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmedWords=[stemmer.stem(w) for w in nltk.word_tokenize(textData)]\n",
    "    return  stemmedWords\n",
    "\n",
    "#filters out only punctation symbols as stop words\n",
    "punctuationList=list(string.punctuation)\n",
    "\n",
    "#initializes Count Vectorizer\n",
    "vect=CountVectorizer(tokenizer=myTokenizer, stop_words=punctuationList,\n",
    "                     max_df=999, max_features=None, min_df=4, binary=True)\n",
    "vect.fit(X_train)\n",
    "\n",
    "#vocabulary_ is a mapping of terms to feature indices\n",
    "print (\"Total vocabulary: \",len(vect.vocabulary_))\n",
    "\n",
    "XTrain = vect.transform(X_train)\n",
    "print(\"XTrain shape:\", XTrain.shape)\n",
    "print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Naive Bayes initialization and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait...\\n\")\n",
    "#trains NaiveBayes on the document-term matrix and corresponding classes\n",
    "clf = MultinomialNB(alpha=1.0)  #alpha 1 for laplace smoothing\n",
    "clf.fit(XTrain, y_train)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3 Classification of newly generated news by the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "\n",
      "Classifications predicted by NB: [0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Please wait...\\n\")\n",
    "\n",
    "yTestNB = []\n",
    "cls_score = 0\n",
    "for i in range(0, len(text_generated)):\n",
    "    news = [[text_generated[i], yTest[i]]]\n",
    "    Test = pd.DataFrame(news, columns = ['News', 'Labels'])\n",
    "    X_test = Test.News\n",
    "    \n",
    "    y_test = Test.Labels\n",
    "    X_test = vect.transform(X_test)\n",
    "\n",
    "    cls_score = clf.score(X_test, y_test)\n",
    "    if(cls_score < 0.5):\n",
    "        if(yTest[i] == 0):\n",
    "            yTestNB.append(1)\n",
    "        else:\n",
    "            yTestNB.append(0)\n",
    "    else:\n",
    "        yTestNB.append(yTest[i])\n",
    "    \n",
    "print('Classifications predicted by NB:', yTestNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Results comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>News text</th>\n",
       "      <th>Random Classes</th>\n",
       "      <th>Classification by NN</th>\n",
       "      <th>Classification by NB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>to learn in life and in baseball. youve had yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>regular season. maxwell said his decision had...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ed so hard to avoid in high school. i respect ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>or late last year.for the full mint interview ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>job is to focus on playing baseball right now....</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           News text  Random Classes  \\\n",
       "0  to learn in life and in baseball. youve had yo...               0   \n",
       "1   regular season. maxwell said his decision had...               1   \n",
       "2  ed so hard to avoid in high school. i respect ...               0   \n",
       "3  or late last year.for the full mint interview ...               1   \n",
       "4  job is to focus on playing baseball right now....               0   \n",
       "\n",
       "   Classification by NN  Classification by NB  \n",
       "0                     0                     0  \n",
       "1                     0                     0  \n",
       "2                     0                     0  \n",
       "3                     0                     0  \n",
       "4                     0                     0  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame({'News text' : text_generated, 'Random Classes' : yTest, 'Classification by NN' : yTestNN, 'Classification by NB' : yTestNB}, \n",
    "                                columns=['News text', 'Random Classes', 'Classification by NN', 'Classification by NB'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
